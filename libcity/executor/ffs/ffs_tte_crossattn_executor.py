import os
import re
import time
import torch
import numpy as np
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from logging import getLogger

from libcity.model.ffs import FFSTTE_AUG
from libcity.utils import ensure_dir, get_evaluator, get_evaluator_no_gat
from libcity.executor.bertlm.scheduler import CosineLRScheduler
from libcity.model.ffs import FFSTTE_CrossAttn
# from libcity.model.ffs.mamba_fuse_view import MambaFuseView
from libcity.model import loss
from tqdm import tqdm

import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt

class FFSTTE_CrossAttn_Executor(object):
    '''
    Pretrain MambaFuseView Model Executor

    '''
    def __init__(self, config, model, roadgat_data):
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.config = config
        self.roadgat_data = roadgat_data

        self._logger = getLogger()

        self.vocab_size = self.roadgat_data.get('vocab_size')
        self.driver_num = self.roadgat_data.get('driver_num')

        self.exp_id = self.config.get('exp_id', None)
        self.max_epochs = self.config.get('max_epoch', 100)
        self.model_name = self.config.get('model', '')
        self.line = self.config.get('line')

        self.learner = self.config.get('learner', 'adamw')
        self.learning_rate = self.config.get('learning_rate', 1e-4)
        self.weight_decay = self.config.get('weight_decay', 0.01)
        self.lr_beta1 = self.config.get('lr_beta1', 0.9)
        self.lr_beta2 = self.config.get('lr_beta2', 0.999)
        self.lr_betas = (self.lr_beta1, self.lr_beta2)
        self.lr_alpha = self.config.get('lr_alpha', 0.99)
        self.lr_epsilon = self.config.get('lr_epsilon', 1e-8)
        self.lr_momentum = self.config.get('lr_momentum', 0)
        self.grad_accmu_steps = self.config.get('grad_accmu_steps', 1)
        self.test_every = self.config.get('test_every', 10)

        self.lr_decay = self.config.get('lr_decay', True)
        self.lr_scheduler_type = self.config.get('lr_scheduler', 'cosinelr')
        self.lr_decay_ratio = self.config.get('lr_decay_ratio', 0.1)
        self.milestones = self.config.get('steps', [])
        self.step_size = self.config.get('step_size', 10)
        self.lr_lambda = self.config.get('lr_lambda', lambda x: x)
        self.lr_T_max = self.config.get('lr_T_max', 30)
        self.lr_eta_min = self.config.get('lr_eta_min', 0)
        self.lr_patience = self.config.get('lr_patience', 10)
        self.lr_threshold = self.config.get('lr_threshold', 1e-4)
        self.lr_warmup_epoch = self.config.get("lr_warmup_epoch", 5)
        self.lr_warmup_init = self.config.get("lr_warmup_init", 1e-6)
        self.t_in_epochs = self.config.get("t_in_epochs", True)

        self.clip_grad_norm = self.config.get('clip_grad_norm', False)
        self.max_grad_norm = self.config.get('max_grad_norm', 1.)
        self.use_early_stop = self.config.get('use_early_stop', False)
        self.patience = self.config.get('patience', 50)
        self.log_every = self.config.get('log_every', 1)
        self.log_batch = self.config.get('log_batch', 10)
        self.saved = self.config.get('saved_model', True)
        self.load_best_epoch = self.config.get('load_best_epoch', True)
        self.l2_reg = self.config.get('l2_reg', None)

        self.node_features = self.roadgat_data.get('node_features')
        self.edge_index = self.roadgat_data.get('edge_index')
        self.edge_index_trans_prob = self.roadgat_data.get('edge_index_trans_prob')
        self.graph_dict = {
            'node_features': self.node_features,
            'edge_index': self.edge_index,
            'edge_index_trans_prob': self.edge_index_trans_prob,
        }

        # cache ensure
        self.cache_dir = './libcity/cache/{}/{}/model_cache'.format(self.line ,self.exp_id)
        self.png_dir = './libcity/cache/{}/{}'.format(self.line, self.exp_id)
        self.evaluate_res_dir = './libcity/cache/{}/{}/evaluate_cache'.format(self.line, self.exp_id)
        self.summary_writer_dir = './libcity/cache/{}/{}'.format(self.line, self.exp_id)
        ensure_dir(self.cache_dir)
        ensure_dir(self.png_dir)
        ensure_dir(self.evaluate_res_dir)
        ensure_dir(self.summary_writer_dir)
        self._writer = SummaryWriter(self.summary_writer_dir)

        self.model = model.to(self.device)

        # æ‰“å°åŸºæœ¬ä¿¡æ¯
        self._logger.info(self.model)
        for name, param in self.model.named_parameters():
            self._logger.info( 'ğŸ§© params \t' + str(name) + '\t' + str(param.shape) + '\t' +
                              str(param.device) + '\t' + str(
                param.requires_grad))

        # -- optimizer , lr_scheduler
        self.optimizer = self._build_optimizer()
        self.lr_scheduler = self._build_lr_scheduler()
        self.optimizer.zero_grad()

        # evaluator ...
        self._logger.info("ğŸ”§ åˆå§‹åŒ–evaluator")
        self.evaluator = get_evaluator_no_gat(self.config)
        self.criterion_mask = torch.nn.NLLLoss(ignore_index=0, reduction='none')
        self.initial_ckpt = self.config.get("initial_ckpt", None)
        self.unload_param = self.config.get("unload_param", [])  # é»˜è®¤ç©º
        if self.initial_ckpt:
            self.load_model_with_initial_ckpt(self.initial_ckpt)

        # ç”¨äº å¯¹æ¯” roadViewå’ŒgpsView è§†å›¾çš„è½¨è¿¹
        self.criterion = torch.nn.CrossEntropyLoss(reduction='none')
        self.contra_loss_type = self.config.get("contra_loss_type", "simclr")
        self.n_views = self.config.get("n_views", 2)
        self.temperature = self.config.get('temperature', 0.05)
        self.similarity = self.config.get('similarity', 'cosine')


        # last for tte
        self.criterion = torch.nn.MSELoss(reduction='none')



    # --------------------- train, eval(eval on eval_dataloader) , test(eval on testLoader)
    def train(self, train_dataloader, eval_dataloader, test_dataloader=None):
        self._logger.info('â³ Start training ...')
        min_val_loss = float('inf')
        wait = 0
        best_epoch = -1
        train_time = []
        eval_time = []
        train_loss = []
        # train_acc = []
        eval_loss = []
        # eval_acc = []
        lr_list = []
        start_epoch = 0


        exp_id = self.config["exp_id"]

        num_batches = len(train_dataloader)
        self._logger.info("â³ Num_batches: train={}, eval={}".format(num_batches, len(eval_dataloader)))

        for epoch_idx in range(start_epoch, self.max_epochs):
            start_time = time.time()
            train_avg_loss = self._train_epoch(train_dataloader, epoch_idx)
            t1 = time.time()
            train_time.append(t1 - start_time)
            train_loss.append(train_avg_loss)
            # train_acc.append(train_avg_acc)
            self._logger.info("epoch complete!")

            self._logger.info("evaluating now!")
            t2 = time.time()
            eval_avg_loss = self._valid_epoch(eval_dataloader, epoch_idx, mode='Eval')
            end_time = time.time()
            eval_time.append(end_time - t2)
            eval_loss.append(eval_avg_loss)
            # eval_acc.append(eval_avg_acc)

            if self.lr_scheduler is not None:
                if self.lr_scheduler_type.lower() == 'reducelronplateau':
                    self.lr_scheduler.step(eval_avg_loss)
                elif self.lr_scheduler_type.lower() == 'cosinelr':
                    self.lr_scheduler.step(epoch_idx + 1)
                else:
                    self.lr_scheduler.step()

            log_lr = self.optimizer.param_groups[0]['lr']
            lr_list.append(log_lr)
            if (epoch_idx % self.log_every) == 0:
                message = 'Epoch [{}/{}] ({})  train_loss: {:.4f}, val_loss: {:.4f}, lr: {:.6f}, {:.2f}s'. \
                    format(epoch_idx, self.max_epochs, (epoch_idx + 1) * num_batches, train_avg_loss,
                           eval_avg_loss, log_lr, (end_time - start_time))
                self._logger.info(message)

            if eval_avg_loss < min_val_loss:
                wait = 0
                if self.saved:
                    model_file_name = self.save_model_with_epoch(epoch_idx)
                    self._logger.info('Val loss decrease from {:.4f} to {:.4f}, '
                                      'saving to {}'.format(min_val_loss, eval_avg_loss, model_file_name))
                min_val_loss = eval_avg_loss
                best_epoch = epoch_idx
            else:
                wait += 1
                if wait == self.patience and self.use_early_stop:
                    self._logger.warning('Early stopping at epoch: %d' % epoch_idx)
                    break

            if (epoch_idx + 1) % self.test_every == 0:
                self.evaluate(test_dataloader)

        if len(train_time) > 0:
            self._logger.info('Trained totally {} epochs, average train time is {:.3f}s, '
                              'average eval time is {:.3f}s'.
                              format(len(train_time), sum(train_time) / len(train_time),
                                     sum(eval_time) / len(eval_time)))
        if self.load_best_epoch:
            self.load_model_with_epoch(best_epoch)

        self._draw_png([(train_loss, eval_loss, 'loss'), (lr_list, 'lr')])
        return min_val_loss

    def _train_epoch(self, train_dataloader, epoch_idx):
        batches_seen = epoch_idx * len(train_dataloader)

        self.model = self.model.train()

        epoch_loss = []  # total loss of epoch
        # total_correct_l = 0  # total top@1 acc for masked elements in epoch
        # total_active_elements_l = 0  # total masked elements in epoch

        for i, batch in tqdm(enumerate(train_dataloader), desc="â³ Train epoch={}".format(
                epoch_idx), total=len(train_dataloader)):
            gps_X, gps_padding_mask, gps_tte_targets, \
                road_X, road_padding_mask, road_traj_mat = batch # gps_tte_targets (B,1)
            predictions = self.model(gps_X, gps_padding_mask, road_X, road_padding_mask, road_traj_mat, self.graph_dict) # (B,1)

            predictions = predictions.squeeze(1) # (B)
            gps_tte_targets = gps_tte_targets.squeeze(1) # (B)


            # road_e, gps_e = self.model(
            #     gps_X = gps_X, gps_padding_mask = gps_padding_mask,
            #     road_X = road_X, padding_masks = road_padding_mask, batch_temporal_mat = road_traj_mat,
            #     graph_dict = self.graph_dict
            # ) # (B, T, vocab_size)
            # targets_l, target_masks_l = road_Target[..., 0], road_target_masks[..., 0] # (B, T), (B, T)
            # mean_loss_l, batch_loss_l, num_active_l = self._cal_loss(predictions_l, targets_l, target_masks_l)
            # mean_loss_con = self._contrastive_loss(z1, z2, self.contra_loss_type)

            batch_loss_list = self.criterion(predictions, gps_tte_targets) # (B)
            batch_loss = torch.sum(batch_loss_list) # 1 scalar
            num_active = len(batch_loss_list)
            mean_loss = batch_loss / num_active

            if self.l2_reg is not None:
                total_loss = mean_loss + self.l2_reg * loss.l2_reg_loss(self.model)
            else:
                total_loss = mean_loss

            total_loss = total_loss / self.grad_accmu_steps
            batches_seen += 1

            # with torch.autograd.detect_anomaly():
            total_loss.backward()

            if self.clip_grad_norm:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)

            if batches_seen % self.grad_accmu_steps == 0:
                # è°ƒæ•´æ¨¡å‹å­¦ä¹ å‚æ•°
                self.optimizer.step()
                if self.lr_scheduler_type == 'cosinelr' and self.lr_scheduler is not None:
                    self.lr_scheduler.step_update(num_updates=batches_seen // self.grad_accmu_steps)
                self.optimizer.zero_grad()

            with torch.no_grad():
                # total_correct_l += self._cal_acc(predictions_l, targets_l, target_masks_l)
                # total_active_elements_l += num_active_l.item()
                epoch_loss.append(mean_loss.item())  # add total loss of batch

            post_fix = {
                "mode": "Train",
                "epoch": epoch_idx,
                "iter": i,
                "lr": self.optimizer.param_groups[0]['lr'],
                'loss': mean_loss.item(),
                # "Loc acc(%)": total_correct_l / total_active_elements_l * 100,
                # "MLM loss": mean_loss_l.item(),
                # "Contrastive loss": mean_loss_con.item(),
            }
            # if self.test_align_uniform or self.train_align_uniform:
            #     post_fix['align_loss'] = align_loss
            #     post_fix['uniform_loss'] = uniform_loss
            if i!=0 and i % self.log_batch == 0:
                print()
                self._logger.info('ğŸ“ˆ ' + str(post_fix))

        epoch_loss = np.mean(epoch_loss)  # average loss per element for whole epoch
        # total_correct_l = total_correct_l / total_active_elements_l * 100.0
        print()
        self._logger.info("ğŸ“ˆ Train: expid = {}, Epoch = {}, avg_loss = {}"
                          .format(self.exp_id, epoch_idx, epoch_loss))
        self._writer.add_scalar('Train loss', epoch_loss, epoch_idx)
        # self._writer.add_scalar('Train loc acc', total_correct_l, epoch_idx)
        return epoch_loss  # , total_correct_l

    def evaluate(self, test_dataloader):
        """
        use model to test dataset

        Args:
            test_dataloader(torch.Dataloader): Dataloader
        """
        self._logger.info('Start evaluating ...')
        start_time = time.time()
        self._valid_epoch(test_dataloader, 0, mode='Test')
        t1 = time.time()
        self._logger.info('Test time {}s.'.format(t1 - start_time))

    def _valid_epoch(self, eval_dataloader, epoch_idx, mode='Eval'):
        self.model = self.model.eval()
        if mode == 'Test':
            self.evaluator.clear()

        epoch_loss = []  # total loss of epoch
        total_active_elements = 0  # total masked elements in epoch
        # total_correct_l = 0  # total top@1 acc for masked elements in epoch
        # total_active_elements_l = 0  # total masked elements in epoch

        labels = []
        preds = []

        with torch.no_grad():
            for i, batch in tqdm(enumerate(eval_dataloader), desc="â³ {} epoch={}".format(
                    mode, epoch_idx), total=len(eval_dataloader)):

                # gps_X, gps_padding_mask, gps_tte_targets, road_X, road_padding_mask, road_traj_mat = batch  # gps_tte_targets (B,1)
                # predictions = self.model(gps_X, gps_padding_mask, road_X, road_padding_mask, road_traj_mat)  # (B,1)

                gps_X, gps_padding_mask, gps_tte_targets, \
                    road_X, road_padding_mask, road_traj_mat = batch  # gps_tte_targets (B,1)
                predictions = self.model(gps_X, gps_padding_mask, road_X, road_padding_mask, road_traj_mat, self.graph_dict)  # (B,1)

                predictions = predictions.squeeze(1)  # (B)
                gps_tte_targets = gps_tte_targets.squeeze(1) # (B)

                if mode == 'Test':
                    preds.append(predictions.cpu().numpy())
                    labels.append(gps_tte_targets.cpu().numpy())
                    evaluate_input = {
                        'y_true': gps_tte_targets,
                        'y_pred': predictions
                    }
                    self.evaluator.collect(evaluate_input)
                    # self._logger.info( self.evaluator.evaluate() )

                batch_loss_list = self.criterion(predictions, gps_tte_targets) # (B)
                batch_loss = torch.sum(batch_loss_list) # 1 scalar
                num_active = len(batch_loss_list)  # batch_size
                mean_loss = batch_loss / num_active  # mean loss (over samples) used for optimization

                epoch_loss.append(mean_loss.item())  # add total loss of batch

                post_fix = {
                    "mode": "Train",
                    "epoch": epoch_idx,
                    "iter": i,
                    "lr": self.optimizer.param_groups[0]['lr'],
                    "loss": mean_loss.item()
                    # "Loc acc(%)": total_correct_l / total_active_elements_l * 100,
                    # "MLM loss": mean_loss_l.item(),
                    # "Contrastive loss": mean_loss_con.item(),
                }
                # if self.test_align_uniform or self.train_align_uniform:
                #     post_fix['align_loss'] = align_loss
                #     post_fix['uniform_loss'] = uniform_loss
                if i!=0 and i % self.log_batch == 0:
                    print()
                    self._logger.info('ğŸ“ˆ '+str(post_fix))

            epoch_loss = np.mean(epoch_loss)  # average loss per element for whole epoch
            # total_correct_l = total_correct_l / total_active_elements_l * 100.0
            print()
            self._logger.info("ğŸ“ˆ {}: expid = {}, Epoch = {}, avg_loss = {}."
                              .format(mode, self.exp_id, epoch_idx, epoch_loss))
            self._writer.add_scalar('{} loss'.format(mode), epoch_loss, epoch_idx)
            # self._writer.add_scalar('{} loc acc'.format(mode), total_correct_l, epoch_idx)

            if mode == 'Test':
                preds = np.concatenate(preds, axis=0)
                labels = np.concatenate(labels, axis=0)
                np.save(self.cache_dir + '/eta_labels.npy', labels)
                np.save(self.cache_dir + '/eta_preds.npy', preds)
                self.evaluator.save_result(self.evaluate_res_dir)
            return epoch_loss # , total_correct_l

    # --------------------- loss
    def _cal_loss(self, pred, targets, targets_mask):
        '''

        Parameters
        ----------
        pred (B, T, V)
        targets (B, T)
        targets_mask(B, T)

        Returns
        -------

        '''
        batch_loss_list = self.criterion_mask(pred.transpose(1, 2), targets.long())
        batch_loss = torch.sum(batch_loss_list)
        num_active = targets_mask.sum()
        mean_loss = batch_loss / num_active  # mean loss (over samples) used for optimization
        return mean_loss, batch_loss, num_active

    def _cal_acc(self, pred, targets, targets_mask):
        '''

        Parameters
        ----------
        pred (B, T, V)
        targets (B, T)
        targets_mask(B, T)

        Returns
        -------

        '''
        mask_label = targets[targets_mask].long()  # (num_active, )
        lm_output = pred[targets_mask].argmax(dim=-1)  # (num_active, )
        correct_l = mask_label.eq(lm_output).sum().item()
        return correct_l

    def align_loss(self, x, y, alpha=2):
        if self.norm_align_uniform:
            x = F.normalize(x, dim=-1)
            y = F.normalize(y, dim=-1)
        return (x - y).norm(p=2, dim=1).pow(alpha).mean()

    def uniform_loss(self, x, t=2):
        if self.norm_align_uniform:
            x = F.normalize(x, dim=-1)
        return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()

    def align_uniform(self, x, y):
        align_loss_val = self.align_loss(x, y, alpha=self.align_alpha)
        unif_loss_val = (self.uniform_loss(x, t=self.unif_t) + self.uniform_loss(y, t=self.unif_t)) / 2
        sum_loss = align_loss_val * self.align_w + unif_loss_val * self.unif_w
        return sum_loss, align_loss_val.item(), unif_loss_val.item()

    def _contrastive_loss(self, z1, z2, loss_type):
        if loss_type == 'simsce':
            return self._contrastive_loss_simsce(z1, z2)
        elif loss_type == 'simclr':
            return self._contrastive_loss_simclr(z1, z2)
        elif loss_type == 'consert':
            return self._contrastive_loss_consert(z1, z2)
        else:
            raise ValueError('Error contrastive loss type {}!'.format(loss_type))

    def _contrastive_loss_simclr(self, z1, z2):
        """

        Args:
            z1(torch.tensor): (batch_size, d_model)
            z2(torch.tensor): (batch_size, d_model)

        Returns:
            loss_mean_res: tensor
        """
        assert z1.shape == z2.shape
        batch_size, d_model = z1.shape
        features = torch.cat([z1, z2], dim=0)  # (batch_size * 2, d_model)

        labels = torch.cat([torch.arange(batch_size) for i in range(self.n_views)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(self.device)

        features = F.normalize(features, dim=1)
        if self.similarity == 'inner':
            similarity_matrix = torch.matmul(features, features.T)
        elif self.similarity == 'cosine':
            similarity_matrix = F.cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim=-1)
        else:
            similarity_matrix = torch.matmul(features, features.T)

        # discard the main diagonal from both: labels and similarities matrix
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        # assert similarity_matrix.shape == labels.shape

        # select and combine multiple positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)  # [batch_size * 2, 1]

        # select only the negatives the negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)  # [batch_size * 2, 2N-2]

        logits = torch.cat([positives, negatives], dim=1)  # (batch_size * 2, batch_size * 2 - 1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)  # (batch_size * 2)
        logits = logits / self.temperature

        # criterion å…¨éƒ¨è®¾ç½®ä¸º reduction='none'
        loss_res = self.criterion(logits, labels) # (2b)
        loss_mean_res = loss_res.sum() / loss_res.shape[0]

        return loss_mean_res

    def _contrastive_loss_simsce(self, z1, z2):
        assert z1.shape == z2.shape
        z1 = F.normalize(z1, dim=-1)
        z2 = F.normalize(z2, dim=-1)
        if self.similarity == 'inner':
            similarity_matrix = torch.matmul(z1, z2.T)
        elif self.similarity == 'cosine':
            similarity_matrix = F.cosine_similarity(z1.unsqueeze(1), z2.unsqueeze(0), dim=-1)
        else:
            similarity_matrix = torch.matmul(z1, z2.T)
        similarity_matrix /= self.temperature

        labels = torch.arange(similarity_matrix.shape[0]).long().to(self.device)
        loss_res = self.criterion(similarity_matrix, labels)
        return loss_res

    # --------------------- Infrastructure
    def _draw_png(self, data):
        '''
        [(train_loss, eval_loss, 'loss'), (train_acc, eval_acc, 'acc'), (lr_list, 'lr')]
        train_loss : list len=$x$,
        eval_loss : list len=$x$,
        lr : titlename
        '''
        for data_iter in data:
            plt.figure()
            if len(data_iter) == 3:
                # ä¸¤æ¡æ›²çº¿ï¼Œä¸€ä¸ªtitle name
                train_list, eval_list, name = data_iter
                x_index = np.arange((len(train_list)))
                plt.plot(x_index, train_list, 'r', label='train_{}'.format(name))
                plt.plot(x_index, eval_list, 'b', label='eval_{}'.format(name))
            else:
                # ä¸€æ¡æ›²çº¿ï¼Œä¸€ä¸ªtitle name
                data_list, name = data_iter
                x_index = np.arange((len(data_list)))
                plt.plot(x_index, data_list, 'r', label='{}'.format(name))
            plt.ylabel(name)
            plt.xlabel('epoch')
            plt.title(str(self.exp_id) + ': ' + str(self.model_name))
            plt.legend()
            path = self.png_dir + '/{}_{}.png'.format(self.exp_id, name)
            plt.savefig(path)
            self._logger.info('Save png at {}'.format(path))

    def _valid_parameter(self, k):
        '''

        Args:
            k: List

        Returns:
            unload_paramä¸­çš„å‚æ•°åœ¨kä¸­å­˜åœ¨ï¼Œ è¿”å›True
            unload_paramä¸­çš„æ‰€æœ‰å‚æ•°åœ¨kä¸­å‡ä¸å­˜åœ¨ï¼Œè¿”å›False

            å¦‚æœkä¸ºstr
            - kä¸ºunload_paramä¸­çš„å‚æ•°ï¼Œ è¿”å›True
            - å¦åˆ™ï¼ŒFalse
        '''
        for para in self.unload_param:
            if para in k:
                return True
        return False

    def load_model_with_initial_ckpt(self, initial_ckpt):
        '''
        åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        - ä»é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ–‡ä»¶initial_ckptä¸­åŠ è½½å‚æ•°åˆ°å½“å‰æ¨¡å‹
        - å¤„ç† modelå’Œ initial_ckptè·å¾—çš„pretrained_model ç›¸äº¤çš„å‚æ•°ï¼ŒåŠ è½½åˆ°modelä¸­

        Args:
            initial_ckpt: path, åˆå§‹æ¨¡å‹ä½ç½®ï¼Œç»§ç»­è®­ç»ƒ

        Returns:

        '''
        assert os.path.exists(initial_ckpt), 'Weights at %s not found' % initial_ckpt
        checkpoint = torch.load(initial_ckpt, map_location='cpu')
        pretrained_model = checkpoint['model'].state_dict()
        # å½“å‰æ¨¡å‹çš„state_dict
        model_keys = self.model.state_dict()
        state_dict_load = {}
        unexpect_keys = []
        # å°†pretrain_modelä¸­å’ŒmodelåŒ¹é…çš„kvåŠ å…¥state_dict_load
        # pretrain âˆ© model
        for k, v in pretrained_model.items():
            # ä¸åœ¨å½“å‰æ¨¡å‹çš„keysä¸­ï¼Œæˆ–è€…ç»´åº¦ä¸åŒ¹é…ï¼Œæˆ–è€…éœ€è¦unloadçš„å‚æ•°
            if k not in model_keys.keys() or v.shape != model_keys[k].shape\
                    or self._valid_parameter(k):
                unexpect_keys.append(k)
            else:
                state_dict_load[k] = v
        # å·®é›†
        # unexcept_keys = (pretrain_model - model) âˆª (model-pretrain_model)
        for k, v in model_keys.items():
            if k not in pretrained_model.keys():
                unexpect_keys.append(k)
        self._logger.info("Unexpected keys: {}".format(unexpect_keys))
        self.model.load_state_dict(state_dict_load, strict=False)
        self._logger.info("Initialize model from {}".format(initial_ckpt))

    def save_model(self, cache_name):
        """
        ä¿å­˜æ¨¡å‹

        Args:
            cache_name(str): ä¿å­˜çš„æ–‡ä»¶å
                # pipeline # ./libcity/cache/{}/model_cache/{}_{}_{}.pt
        """
        ensure_dir(self.cache_dir)
        config = dict()
        if isinstance(self.model, FFSTTE_CrossAttn):
            config['model'] = self.model.cpu()
            config['optimizer_state_dict'] = self.optimizer.state_dict()
            torch.save(config, cache_name)
            self.model.to(self.device)
            self._logger.info("ğŸ§Š Saved model at" + cache_name)
        else:
            self._logger.info("âŒ modeléMambaFuseView")
            raise RuntimeError("âŒ modeléMambaFuseView")

    def load_model_state(self, cache_name):
        """
        åŠ è½½å¯¹åº”æ¨¡å‹çš„ cache ï¼ˆç”¨äºåŠ è½½å‚æ•°ç›´æ¥è¿›è¡Œæµ‹è¯•çš„åœºæ™¯ï¼‰

        Args:
            cache_name(str): ä¿å­˜çš„æ–‡ä»¶å
                # pipeline # ./libcity/cache/{}/model_cache/{}_{}_{}.pt
        """
        assert os.path.exists(cache_name), 'Weights at {} not found' % cache_name
        checkpoint = torch.load(cache_name, map_location='cpu')
        if isinstance(self.model, FFSTTE_CrossAttn):
            self.model.load_state_dict(checkpoint['model'].state_dict()) # åŠ è½½åˆ°å½“å‰model
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self._logger.info("ğŸ”¥ Loaded model at " + cache_name)
        else:
            self._logger.info("âŒ modeléMambaFuseView")
            raise RuntimeError("âŒ modeléMambaFuseView")

    def load_model(self, cache_name):
        """
        åŠ è½½å¯¹åº”æ¨¡å‹çš„ cache

        Args:
            cache_name(str): ä¿å­˜çš„æ–‡ä»¶å
        """
        assert os.path.exists(cache_name), 'Weights at {} not found' % cache_name
        checkpoint = torch.load(cache_name, map_location='cpu')
        if isinstance(self.model, FFSTTE_AUG):
            self.model = checkpoint['model'].to(self.device) # ç›´æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¦†ç›–model
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self._logger.info("ğŸ”¥ Loaded model at " + cache_name)
        else:
            self._logger.info("âŒ modeléMambaFuseView")
            raise RuntimeError("âŒ modeléMambaFuseView")

    def save_model_with_epoch(self, epoch):
        """
        ä¿å­˜æŸä¸ªepochçš„æ¨¡å‹

        Args:
            epoch(int): è½®æ•°
        """
        ensure_dir(self.cache_dir) # './libcity/cache/{}/model_cache'.format(self.exp_id)
        config = dict()
        if isinstance(self.model, FFSTTE_CrossAttn):
            config['model'] = self.model.cpu()
            config['optimizer_state_dict'] = self.optimizer.state_dict()
            config['epoch'] = epoch
            model_path = self.cache_dir + '/' + self.config['model'] + '_' + self.config['dataset'] + '_epoch%d.tar' % epoch
            torch.save(config, model_path)
            self.model.to(self.device)
            self._logger.info("ğŸ§Š Saved model at {}".format(epoch))
            return model_path
        else:
            self._logger.info("âŒ model é MambaFuseView")
            raise RuntimeError("âŒ modeléMambaFuseView")

    def load_model_with_epoch(self, epoch):
        """
        åŠ è½½æŸä¸ªepochçš„æ¨¡å‹

        Args:
            epoch(int): è½®æ•°
        """
        model_path = self.cache_dir + '/' + self.config['model'] + '_' + self.config['dataset'] + '_epoch%d.tar' % epoch
        assert os.path.exists(model_path), 'Weights at epoch %d not found' % epoch
        if isinstance(self.model, FFSTTE_CrossAttn):
            checkpoint = torch.load(model_path, map_location='cpu')
            self.model = checkpoint['model'].to(self.device)
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self._logger.info("ğŸ”¥ Loaded model at {}".format(epoch))
        else:
            self._logger.info("âŒ model é MambaFuseView")
            raise RuntimeError("âŒ modeléMambaFuseView")

    def _build_optimizer(self):
        """
        æ ¹æ®å…¨å±€å‚æ•°`learner`é€‰æ‹©optimizer
        """
        self._logger.info('âœ… Use Optimizer `{}` .'.format(self.learner.lower()))
        if self.learner.lower() == 'adam':
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate,
                                         eps=self.lr_epsilon, betas=self.lr_betas, weight_decay=self.weight_decay)
        elif self.learner.lower() == 'adamw': #
            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate,
                                          eps=self.lr_epsilon, betas=self.lr_betas, weight_decay=self.weight_decay)
        elif self.learner.lower() == 'sgd':
            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate,
                                        momentum=self.lr_momentum, weight_decay=self.weight_decay)
        elif self.learner.lower() == 'adagrad':
            optimizer = torch.optim.Adagrad(self.model.parameters(), lr=self.learning_rate,
                                            eps=self.lr_epsilon, weight_decay=self.weight_decay)
        elif self.learner.lower() == 'rmsprop':
            optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.learning_rate,
                                            alpha=self.lr_alpha, eps=self.lr_epsilon,
                                            momentum=self.lr_momentum, weight_decay=self.weight_decay)
        elif self.learner.lower() == 'sparse_adam':
            optimizer = torch.optim.SparseAdam(self.model.parameters(), lr=self.learning_rate,
                                               eps=self.lr_epsilon, betas=self.lr_betas)
        else:
            self._logger.warning('Received unrecognized optimizer, set default Adam optimizer')
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate,
                                         eps=self.lr_epsilon, weight_decay=self.weight_decay)
        return optimizer

    def _build_lr_scheduler(self):
        """
        æ ¹æ®å…¨å±€å‚æ•°`lr_scheduler`é€‰æ‹©å¯¹åº”çš„lr_scheduler
        """
        if self.lr_decay:
            self._logger.info('âœ… Use lr_scheduler `{}` .'.format(self.lr_scheduler_type.lower()))
            if self.lr_scheduler_type.lower() == 'multisteplr':
                lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(
                    self.optimizer, milestones=self.milestones, gamma=self.lr_decay_ratio)
            elif self.lr_scheduler_type.lower() == 'steplr':
                lr_scheduler = torch.optim.lr_scheduler.StepLR(
                    self.optimizer, step_size=self.step_size, gamma=self.lr_decay_ratio)
            elif self.lr_scheduler_type.lower() == 'exponentiallr':
                lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
                    self.optimizer, gamma=self.lr_decay_ratio)
            elif self.lr_scheduler_type.lower() == 'cosineannealinglr':
                lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer, T_max=self.lr_T_max, eta_min=self.lr_eta_min)
            elif self.lr_scheduler_type.lower() == 'lambdalr':
                lr_scheduler = torch.optim.lr_scheduler.LambdaLR(
                    self.optimizer, lr_lambda=self.lr_lambda)
            elif self.lr_scheduler_type.lower() == 'reducelronplateau':
                lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizer, mode='min', patience=self.lr_patience,
                    factor=self.lr_decay_ratio, threshold=self.lr_threshold)
            elif self.lr_scheduler_type.lower() == 'cosinelr': #
                lr_scheduler = CosineLRScheduler(
                    self.optimizer, t_initial=self.max_epochs, lr_min=self.lr_eta_min, decay_rate=self.lr_decay_ratio,
                    warmup_t=self.lr_warmup_epoch, warmup_lr_init=self.lr_warmup_init, t_in_epochs=self.t_in_epochs)
            else:
                self._logger.warning('Received unrecognized lr_scheduler, '
                                     'please check the parameter `lr_scheduler`.')
                lr_scheduler = None
        else:
            lr_scheduler = None
        return lr_scheduler
